experiment:
  name: "single_MAE_fancy" #"single-10x-ssim-mse" # "cross-val-v8"
  save_dir: "/scratch/fquareng/experiments"
  device: "cuda"  # "cuda" or "cpu"
  vars: ["T_2M", "TOT_PREC"]

paths:
  local_dir: "/work/FAC/FGSE/IDYST/tbeucler/downscaling/fquareng/WeatherAdaptSR"
  data_path: "/work/FAC/FGSE/IDYST/tbeucler/downscaling/fquareng/data/clusters_v5"
  elev_path: "/work/FAC/FGSE/IDYST/tbeucler/downscaling/fquareng/data/dem_squares"
  clusters: [
    "cluster_0",
    "cluster_1",
    "cluster_2",
    "cluster_3",
    "cluster_4",
    "cluster_5",
    "cluster_6",
    "cluster_7",
    # "cluster_8",
    # "cluster_9",
    # "cluster_10",
    # "cluster_11",
  ]
  # single_cluster: "cluster_0"

optimization:
  num_epochs: 10
  num_trials: 10

training:
  augmentation: False
  num_epochs: 200
  batch_size: 128
  use_theta_e: False
  load_data_on_gpu: True
  num_workers: 0  # 0 if on GH200
  early_stopping: True
  early_stopping_params:
    patience: 10
    rolling_mean_threshold: 5
    rolling_mean_window: 5
  criterion: L1Loss # MSELoss
  structural_criterion: MultiScaleStructuralSimilarityIndexMeasure
  structural_criterion_params:
    betas: [0.0448, 0.2856, 0.3001]
  loss_weights:
    alpha: 50
  optimizer: Adam
  scheduler: ReduceLROnPlateau
  scheduler_params:
    mode: 'min'  # 'min' for MSELoss/L1Loss, 'max' for accuracy
    factor: 0.1
    patience: 5
  mmd:
    lambda_max: 1

testing:
  structural_criterion: MultiScaleStructuralSimilarityIndexMeasure
  criterion: L1Loss # MSELoss

domain_specific:
  cluster_0:
    optimizer_params:
      lr_model: 0.00010103784632843117
      weight_decay: 2.7081903354200482e-06
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  cluster_1:
    optimizer_params:
      lr_model: 1.741162886898334e-06
      weight_decay: 1.357048098375383e-06
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  cluster_10:
    optimizer_params:
      lr_model: 0.00012393557885107537
      weight_decay: 1.183175858041721e-05
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  cluster_11:
    optimizer_params:
      lr_model: 6.929905784207916e-05
      weight_decay: 2.5955438444608163e-05
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  cluster_2:
    optimizer_params:
      lr_model: 7.2939581804924195e-06
      weight_decay: 1.0129210796743568e-05
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  cluster_3:
    optimizer_params:
      lr_model: 1.0752183855103428e-06
      weight_decay: 7.984340926334343e-05
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  cluster_4:
    optimizer_params:
      lr_model: 6.516305481847179e-06
      weight_decay: 1.0818494064318493e-05
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  cluster_5:
    optimizer_params:
      lr_model: 2.032703875649766e-05
      weight_decay: 3.2284575168535546e-05
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  cluster_6:
    optimizer_params:
      lr_model: 4.951768649864631e-06
      weight_decay: 7.849846187099783e-06
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  cluster_7:
    optimizer_params:
      lr_model: 1.5245527478222927e-05
      weight_decay: 0.00017318314906004183
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  cluster_8:
    optimizer_params:
      lr_model: 3.60232484808421e-06
      weight_decay: 4.580783173280096e-06
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  cluster_9:
    optimizer_params:
      lr_model: 4.580079928910211e-06
      weight_decay: 0.00012448253031874595
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0
  all_clusters:
    optimizer_params:
      lr_model: 0.0001
      weight_decay: 1.0e-05
    loss_params:
      lr_loss: 0.0001
      weight_decay: 0.0